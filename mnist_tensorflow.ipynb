{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bk073/TensorFlow-Tutorials/blob/master/mnist_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhvb1xvn2EVx",
        "colab_type": "text"
      },
      "source": [
        "The MNIST data has handwritten digits from 0â€“9 with 60,000 images for training\n",
        "and 10,000 images for testing\n",
        "\n",
        "The images are normalized to the size of 28 image pixels by 28\n",
        "image pixels, converted to grey size, and centered to a fixed size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGv5UBg23AXu",
        "colab_type": "text"
      },
      "source": [
        "Load the MNIST data directly from TensorFlow. Note that we specify one hot\n",
        "encoding as an argument while loading the data. The label is stored as integers\n",
        "but should be loaded as one-hot encoding in-order to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0R6wk9s2NPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "645a3a48-6bf1-45f6-fb6b-65e6431a0007"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDil9Lk4P2a",
        "colab_type": "text"
      },
      "source": [
        "A placeholder is a tensor where the **data is passed**. **Placeholders aren't specific\n",
        "values but will receive input during computation .** \n",
        "\n",
        "x_input is the input where the images will be fed later.\n",
        "y_input is the placeholder where the one-shot labels or targets will be supplied.\n",
        "\n",
        "The None in the shape argument indicates that it can be of any size as we have not\n",
        "yet defined the batch size.**bold text**\n",
        "\n",
        "None = Dynamic shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpyoPjxM21aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 64 * 64 = 784 i.e dimension of image. Now the input layer must have 784 nodes\n",
        "\n",
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 200\n",
        "\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7hu2VO_5Hg6",
        "colab_type": "text"
      },
      "source": [
        "**Placeholder**\n",
        "\n",
        "Blog:  https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363\n",
        "\n",
        " Its value must be fed using the feed_dict optional argument to Session.run(), Tensor.eval(), or Operation.run().\n",
        " \n",
        " sess.run(y, feed_dict={x: rand_array})\n",
        " \n",
        " source: docs\n",
        " \n",
        "Blog : **To access a dynamic shape value, you need to run your graph and feed any placeholder that your tensor my depended upon:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VysQM50QC8y_",
        "colab_type": "text"
      },
      "source": [
        "The values of these variables will be learned during\n",
        "computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJCIm6Pw3008",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
        "bias = tf.Variable(tf.random_normal([no_classes]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjTViaadDFnG",
        "colab_type": "text"
      },
      "source": [
        "The inputs are then weighted and added with the bias to\n",
        "produce logits as shown next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnvGC4Q1DGnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = tf.matmul(x_input, weights) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqSzVzcDZsW",
        "colab_type": "text"
      },
      "source": [
        "**Loss function**\n",
        "\n",
        "The loss can be computed by averaging the cross-entropies. Then the cross-\n",
        "entropy is fed through gradient descent optimization done\n",
        "by tf.train.GradientDescentOptimizer . The optimizer takes the loss and minimizes it\n",
        "with a learning rate of 0.5 . The computation of softmax, cross-entropy, loss,\n",
        "optimization is shown next:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "123a2ABwlHHp",
        "colab_type": "text"
      },
      "source": [
        "**tf.nn.softmax_cross_entropy_with_logits**\n",
        "\n",
        "-> it is better to use softmax coupled\n",
        "with cross-entropy for comparing logits and one-hot labels.\n",
        "\n",
        "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class).\n",
        "\n",
        "NOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSa5eDqiDPId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6067acae-d577-4218-f668-7389bbe6dbd2"
      },
      "source": [
        "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=logits)\n",
        "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
        "optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0711 15:13:19.197251 140649360131968 deprecation.py:323] From <ipython-input-9-e908baccd1c9>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-vkPr8Umc4_",
        "colab_type": "text"
      },
      "source": [
        "**Training model ** and **Session**\n",
        "\n",
        "Initialize the variables using a global variable initializer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UvCSzZYmInF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfAM4xl5msSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "119668b2-4b85-4b74-dc67-55cbb5c4e819"
      },
      "source": [
        "for batch_no in range(total_batches):\n",
        "  mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "  loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
        "      x_input: mnist_batch[0],\n",
        "      y_input: mnist_batch[1]\n",
        "  })\n",
        "  print(loss_value)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 13.149568]\n",
            "[None, 12.686928]\n",
            "[None, 11.001388]\n",
            "[None, 8.908009]\n",
            "[None, 9.513539]\n",
            "[None, 8.781652]\n",
            "[None, 7.209237]\n",
            "[None, 5.9131727]\n",
            "[None, 6.033996]\n",
            "[None, 6.26871]\n",
            "[None, 5.2980742]\n",
            "[None, 4.4977183]\n",
            "[None, 5.415235]\n",
            "[None, 5.1726627]\n",
            "[None, 5.2123895]\n",
            "[None, 4.983762]\n",
            "[None, 4.6331716]\n",
            "[None, 5.0341663]\n",
            "[None, 3.4482338]\n",
            "[None, 4.361772]\n",
            "[None, 3.5199456]\n",
            "[None, 3.4030077]\n",
            "[None, 3.4574268]\n",
            "[None, 2.7100787]\n",
            "[None, 2.1369193]\n",
            "[None, 3.512426]\n",
            "[None, 3.9308865]\n",
            "[None, 2.240956]\n",
            "[None, 2.138719]\n",
            "[None, 2.4432073]\n",
            "[None, 2.5776627]\n",
            "[None, 2.6466565]\n",
            "[None, 2.6281087]\n",
            "[None, 2.2433355]\n",
            "[None, 2.4506707]\n",
            "[None, 2.4943836]\n",
            "[None, 2.1070054]\n",
            "[None, 1.8580148]\n",
            "[None, 2.1389234]\n",
            "[None, 2.1950686]\n",
            "[None, 2.02647]\n",
            "[None, 2.711564]\n",
            "[None, 2.3337216]\n",
            "[None, 2.4277363]\n",
            "[None, 2.0814385]\n",
            "[None, 2.0795078]\n",
            "[None, 2.446528]\n",
            "[None, 1.5046741]\n",
            "[None, 2.065237]\n",
            "[None, 2.1206264]\n",
            "[None, 1.9071944]\n",
            "[None, 1.4844735]\n",
            "[None, 1.833798]\n",
            "[None, 1.6980351]\n",
            "[None, 1.6585736]\n",
            "[None, 2.3372726]\n",
            "[None, 0.91916704]\n",
            "[None, 1.4004407]\n",
            "[None, 1.3646812]\n",
            "[None, 1.8306007]\n",
            "[None, 1.3378578]\n",
            "[None, 1.9209073]\n",
            "[None, 2.0870216]\n",
            "[None, 1.5848684]\n",
            "[None, 1.4873359]\n",
            "[None, 2.0349815]\n",
            "[None, 1.9424297]\n",
            "[None, 1.1121356]\n",
            "[None, 2.3686783]\n",
            "[None, 1.7722098]\n",
            "[None, 1.7147076]\n",
            "[None, 1.7655172]\n",
            "[None, 1.6042463]\n",
            "[None, 2.5371046]\n",
            "[None, 1.5590072]\n",
            "[None, 1.2745305]\n",
            "[None, 1.3595529]\n",
            "[None, 1.8340055]\n",
            "[None, 1.7570456]\n",
            "[None, 1.7979358]\n",
            "[None, 1.672233]\n",
            "[None, 1.1541637]\n",
            "[None, 1.8879808]\n",
            "[None, 1.6357231]\n",
            "[None, 1.2401211]\n",
            "[None, 1.4521245]\n",
            "[None, 1.3195971]\n",
            "[None, 1.6965314]\n",
            "[None, 1.7355391]\n",
            "[None, 1.301101]\n",
            "[None, 0.7729116]\n",
            "[None, 1.7903215]\n",
            "[None, 1.7852695]\n",
            "[None, 1.2536516]\n",
            "[None, 1.0931877]\n",
            "[None, 1.3842369]\n",
            "[None, 1.5678413]\n",
            "[None, 1.3621798]\n",
            "[None, 1.5343521]\n",
            "[None, 1.681319]\n",
            "[None, 1.0188777]\n",
            "[None, 1.6338464]\n",
            "[None, 1.2826465]\n",
            "[None, 1.124484]\n",
            "[None, 1.0061299]\n",
            "[None, 0.9661937]\n",
            "[None, 1.4888043]\n",
            "[None, 1.3442656]\n",
            "[None, 1.4726521]\n",
            "[None, 0.87537986]\n",
            "[None, 1.4134088]\n",
            "[None, 1.0637217]\n",
            "[None, 1.2287658]\n",
            "[None, 1.2551311]\n",
            "[None, 1.3250388]\n",
            "[None, 0.7184422]\n",
            "[None, 1.5072829]\n",
            "[None, 0.97383887]\n",
            "[None, 1.9439162]\n",
            "[None, 0.9394194]\n",
            "[None, 1.3822867]\n",
            "[None, 1.5198382]\n",
            "[None, 0.779556]\n",
            "[None, 0.9913531]\n",
            "[None, 0.9804561]\n",
            "[None, 1.7216183]\n",
            "[None, 1.0871973]\n",
            "[None, 1.6395384]\n",
            "[None, 1.3600479]\n",
            "[None, 0.83984077]\n",
            "[None, 1.5007199]\n",
            "[None, 1.0337814]\n",
            "[None, 1.4272386]\n",
            "[None, 1.1910636]\n",
            "[None, 1.2415669]\n",
            "[None, 1.2700316]\n",
            "[None, 0.6797685]\n",
            "[None, 1.5077643]\n",
            "[None, 1.1223171]\n",
            "[None, 0.84919256]\n",
            "[None, 0.6478171]\n",
            "[None, 1.1944494]\n",
            "[None, 1.0073086]\n",
            "[None, 1.2332172]\n",
            "[None, 1.2996423]\n",
            "[None, 1.6282709]\n",
            "[None, 1.5827578]\n",
            "[None, 0.93372613]\n",
            "[None, 1.4410939]\n",
            "[None, 0.77344465]\n",
            "[None, 1.2993231]\n",
            "[None, 0.85995257]\n",
            "[None, 1.0894164]\n",
            "[None, 1.63665]\n",
            "[None, 1.0811086]\n",
            "[None, 1.1619059]\n",
            "[None, 1.4573475]\n",
            "[None, 0.9661208]\n",
            "[None, 1.185361]\n",
            "[None, 1.2625202]\n",
            "[None, 1.0279502]\n",
            "[None, 1.1013694]\n",
            "[None, 1.6116607]\n",
            "[None, 1.1851726]\n",
            "[None, 1.098458]\n",
            "[None, 1.4363393]\n",
            "[None, 1.5378078]\n",
            "[None, 0.7658369]\n",
            "[None, 1.4476734]\n",
            "[None, 1.4312849]\n",
            "[None, 1.1882343]\n",
            "[None, 1.1732986]\n",
            "[None, 1.2377708]\n",
            "[None, 0.8439465]\n",
            "[None, 0.8446123]\n",
            "[None, 1.4624802]\n",
            "[None, 1.2498056]\n",
            "[None, 0.75659865]\n",
            "[None, 0.757758]\n",
            "[None, 1.5512007]\n",
            "[None, 1.2307487]\n",
            "[None, 1.4170196]\n",
            "[None, 0.91937774]\n",
            "[None, 0.8071132]\n",
            "[None, 1.0053833]\n",
            "[None, 1.076945]\n",
            "[None, 1.1900272]\n",
            "[None, 0.9430074]\n",
            "[None, 1.0158086]\n",
            "[None, 1.055995]\n",
            "[None, 0.7784908]\n",
            "[None, 1.5610397]\n",
            "[None, 0.91482973]\n",
            "[None, 0.88584745]\n",
            "[None, 1.2719924]\n",
            "[None, 1.0409381]\n",
            "[None, 0.9996405]\n",
            "[None, 0.99025816]\n",
            "[None, 1.330815]\n",
            "[None, 0.72690284]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06bwBF5XnhJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8015d2bf-e120-410b-9f27-1db98c4c4541"
      },
      "source": [
        "predictions = tf.argmax(logits, 1)\n",
        "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
        "    x_input: test_images,\n",
        "    y_input: test_labels\n",
        "})\n",
        "print('Accuracy:', accuracy_value)\n",
        "session.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0zAQ3robvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}